<html>
	<head>
		<meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
		<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Sofia">
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@300&display=swap" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@1,500&display=swap" rel="stylesheet">
		<link rel="stylesheet" href="css/base_style.css">
		<link rel="icon" type="image/x-icon" href="images/i-SpeakR_icon2.ico">
		<title>i-SpeakR</title>
	</head>

	<body lang="en-IN">

		<div class="flex-container title_pane">
			<img src="images/gov-logo-1.png" alt="MeitY, GoI, logo" title="MeitY, GoI">
			<img src="images/i-SpeakR_icon2.bmp" alt="i-SpeakR logo" title="i-SpeakR" height="75px">
		</div>

		<div class="flex-container menu_pane">
			<div class="menu_items hyperlink" onclick="window.open('index.html', '_self');">
				Home
			</div>
			&emsp;
			<div class="menu_items hyperlink" onclick="window.open('about.html', '_self');">
				About
			</div>
			&emsp;
			<div class="menu_items hyperlink" onclick="window.open('resources.html', '_self');">
				Resources
			</div>
			&emsp;
			<div class="menu_items hyperlink" onclick="window.open('https://indicasv.iitdh.ac.in/', '_self');"> <!-- https://indicasv.iitdh.ac.in/ -->
				Demo
			</div>
			&emsp;
			<div class="menu_items hyperlink" onclick="window.open('hands_on.html', '_self');">
				Hands-on
			</div>
			<div class="empty_menu_item">
			</div>
		</div>

		<div class="flex-container dashboard">
			<div class="content_space">				
				<div class="heading1">I-MLSV 2022</div>
				<p>Indic-Multilingual Speaker Verification Challenge 2022</p>
				<div class="subsection">
					<div class="text">



				<div class="heading2">ABOUT the Challenge</div>





   
        	Speaker verification (SV) is a task of verifying whether an input utterance matches the claimed identity.  Though there exists ample amount of research in the development of SV technologies, the development with respect to multilingual aspects is very few. Further In a country like India almost all the speakers are multilingual, hence the data collected from them are challenging and can pose a true motivation for the development of multilingual SV (MLSV) system. With this motivation, the COCOSDA INDIC- Multilingual Speaker Verification (I-MLSV) Challenge 2022 has been designed for understanding and comparing the research on SV techniques using speech data from 13 Indian languages collected in five different sensors. The goal of this challenge is to make the SV system robust from language and sensor variations. 
 Challenge Rules
<ul>
	<li>Submissions other than the defined tasks will not be included in this challenge.</li>
	<li>Participants cannot use audio data other than the audio data released as a part of this challenge for the constrained speaker verification, however for the unconstrained speaker verification participants are free to use audio data other than the audio data released as a part of this challenge as long as the audio data is publicly available.</li>
	<li>EER(Equal Error Rate) will be used as the metric for evaluation in all the defined scenarios</li>
	<li>Teams will need to share their final Speaker verification (SV) model, along with a write up in o-cocosda format (https://vlsp.org.vn/cocosda2022/paper-submission), which should give a brief description about
		<ul>
		<li>The database used with appropriate citations</li>
		<li>Brief description of the methods  used to build model</li>
		<li>Github link  with proper code structure and details</li>
		</ul>
	<li>Nature of Data set
	<ul>
	<li>Development/Training data: The common training data set consists of utterances from Indian languages, collected in multiple sessions using 5 different sensors.</li>
	<li>Enrolment data: The Enrolment data consists of utterances from English language captured in multiple sessions using only headset (H01) sensor.</li>
	<li>Public test data: Public test data will be provided in two conditions Matched and Mismatched
		<ul>
		<li>In matched conditions the test utterances will come from the same language and also from the same sensor.</li>
		<li>In mismatched conditions the test utterances will come from different languages and different sensors.</li>
		</li>
		</ul>
	<li>Private test data (10 to 60 sec): In private test data, the test utterances will come from cross languages and also from all five sensors. (Mismatched condition)
Tracks
		<ul>
		<li>Track 1: Constrained speaker verification
			<ul>
			<li>Development data: The development data will be provided as defined in above section heading (Nature of dataset)</li>
			<li>Enrolment data: The enrolment data will be provided as defined in the above section heading (Nature of dataset)</li>
			<li>Test data: The test utterances will come from different languages and different sensors. (Mismatched condition)</li>
			</li>
			</ul>
		<li>Track 2: Unconstrained speaker verification
			<ul>
			<li>Participants are free to use audio data for training other than the audio data released as a part of this challenge as long as the audio data is publicly available.</li>
			<li>Once the model is ready participants can test their model using the private test data defined above</li>
			 </li>
			</ul>
		</ul>

	<li>Metrics for evaluation: The performance of the models will be evaluated by the Equal Error Rate (EER) where the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR)</li>

	<li>Submission Guidelines
<ul>
<li>Multiple submissions are allowed but under a limitation of each phase, the evaluation result is based on the submission having the lowest EER.
<li>The submission file comprises a header; a single line must contain 3 fields separated by tab character in the following format:
test_wav<TAB>target_speaker_ID<TAB>similarity score<NEWLINE>
**where
test_wav - The test utterance
score - The cosine similarity
For example:
test_wav   	   	target_speaker_ID      	score
1001_AD01OENC.wav  	1001       	 	0.97285
 
Timeline
Challenge registration open: July 25th, 2022
Release of training data: September 7th, 2022
Release of validation and test data: October 15th, 2022
Announcing the Top 3 to be presented at the conference:October 20th, 2022
Announcing the ranking winners: November 24-26th, 2022

Organizer: 
	Team NLTM Speaker recognition, India 
Details about the team: https://sr-meity.github.io/Manuals/
Contact:
Any queries kindly write to us in jagabandhu.mishra.18@iitdh.ac.in

 















					</div>
				</div>

			</div>
		</div>

		<div class="flex-container footer">
			<div>Â© Designed and maintained by IIT Dharwad.</div>
		</div>

	</body>
</html>
